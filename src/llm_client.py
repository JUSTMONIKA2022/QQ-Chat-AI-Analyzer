# src/llm_client.py

"""
LLM Client Module (Phase 2)
===========================
è´Ÿè´£ä¸ LLM API äº¤äº’ã€‚
éµå¾ª Phase 5 ç¼–ç¨‹è§„èŒƒã€‚
"""

import os
from typing import Dict, Any, List, Optional
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

from src.registry import *

class LLMClient:
    """
    LLM å®¢æˆ·ç«¯ï¼Œæ”¯æŒé»˜è®¤é…ç½®ä¸è‡ªå®šä¹‰é…ç½®åŒæ¨¡å¼ã€‚
    """

    def __init__(self, mode: str = LLM_MODE_DEFAULT, api_key: str = None, base_url: str = DEFAULT_API_BASE, model: str = DEFAULT_MODEL):
        # æ„ä¹‰: åˆå§‹åŒ–å®¢æˆ·ç«¯
        # ä½œç”¨: åŠ è½½ API Key å’Œ Base URL
        # å…³è”: è¢«ä¸»ç¨‹åºè°ƒç”¨
        
        self.mode = mode
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.client = None
        
        if mode == LLM_MODE_DEFAULT and not self.api_key:
            # é»˜è®¤æ¨¡å¼ï¼šå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
            self.api_key = os.environ.get("OPENAI_API_KEY", "DEMO_KEY")
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (å¦‚æœ Key æœ‰æ•ˆä¸”åº“å·²å®‰è£…)
        if OpenAI and self.api_key and self.api_key != "DEMO_KEY":
            try:
                self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            except Exception as e:
                print(f"[Warning] Failed to init OpenAI client: {e}")

    def generate_summary(self, text_content: str) -> str:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        system_prompt = self.build_system_prompt("è¯·ç”Ÿæˆä¸€ä»½å¹½é»˜çš„å¹´åº¦æ€»ç»“æŠ¥å‘Šï¼ŒåŒ…å«ï¼šå¹´åº¦ç¾¤ç”»åƒã€å­£åº¦å°å‰§åœºã€å¹´åº¦é¢å¥–å…¸ç¤¼ã€ç¤¾æ­»æ—¶åˆ»ã€å¹´åº¦æ€»ç»“è¯—ã€‚")
        return self.chat_completion(system_prompt, f"ä»¥ä¸‹æ˜¯éƒ¨åˆ†èŠå¤©è®°å½•é‡‡æ ·ï¼š\n{text_content}")

    def analyze_sentiment(self, text_content: str) -> str:
        """ç”Ÿæˆæƒ…æ„Ÿåˆ†æ"""
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å¯¹è¯çš„æƒ…æ„ŸåŸºè°ƒï¼Œå¹¶ç»™å‡ºç§¯æ/æ¶ˆæ/ä¸­æ€§è¯„ä»·ï¼Œä»¥åŠå…³é”®çš„æƒ…ç»ªè§¦å‘ç‚¹ã€‚è¯·ç›´æ¥è¿”å› HTML ç‰‡æ®µã€‚"
        return self.chat_completion(system_prompt, f"ä»¥ä¸‹æ˜¯éƒ¨åˆ†èŠå¤©è®°å½•é‡‡æ ·ï¼š\n{text_content}")

    def chat_completion(self, system_prompt: str, user_prompt: str, model: Optional[str] = None) -> str:
        """
        è°ƒç”¨ LLM Chat Completion APIã€‚
        """
        # æ„ä¹‰: å‘é€è¯·æ±‚
        # ä½œç”¨: å°è£… OpenAI SDK è°ƒç”¨ï¼Œå¤„ç†ç½‘ç»œå¼‚å¸¸
        # å…³è”: æ ¸å¿ƒ AI åŠŸèƒ½å…¥å£
        
        target_model = model if model else self.model
        
        # 1. å°è¯•çœŸå®è°ƒç”¨
        if self.client:
            # ç®€å•é‡è¯•æœºåˆ¶ (Max 2 times)
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    print(f"[Info] Sending request to {target_model} (Attempt {attempt+1}/{max_retries})...")
                    response = self.client.chat.completions.create(
                        model=target_model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        timeout=60  # è®¾ç½® 60s è¶…æ—¶
                    )
                    content = response.choices[0].message.content
                    if not content:
                        raise ValueError("Empty response from LLM")
                    return content
                    
                except Exception as e:
                    error_msg = str(e)
                    print(f"[Error] API Call Failed (Attempt {attempt+1}): {error_msg}")
                    
                    # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œä¸”æ˜¯è‡ªå®šä¹‰æ¨¡å¼ï¼Œåˆ™è¿”å›é”™è¯¯ UI
                    if attempt == max_retries - 1:
                        if self.mode != LLM_MODE_DEFAULT:
                            print(f"[Error] All retries failed. Returning error message to UI.")
                            return f"""
                            <div style="border: 2px solid #ff4444; padding: 15px; background: #fff0f0; color: #cc0000; border-radius: 8px; margin: 20px 0; font-family: sans-serif;">
                                <h3 style="margin-top:0; color: #cc0000;">âš ï¸ AI ç”Ÿæˆå¤±è´¥ (API Error)</h3>
                                <div style="margin-bottom: 10px;">
                                    <strong>é”™è¯¯ä¿¡æ¯:</strong> <code style="background: #eee; padding: 2px 5px; border-radius: 4px;">{error_msg}</code>
                                </div>
                                <ul style="padding-left: 20px; color: #666;">
                                    <li><strong>æ¨¡å‹:</strong> {target_model}</li>
                                    <li><strong>åœ°å€:</strong> {str(self.client.base_url)}</li>
                                    <li><strong>å»ºè®®:</strong> è¯·æ£€æŸ¥ API Key ä½™é¢ã€ç½‘ç»œè¿é€šæ€§æˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚</li>
                                </ul>
                            </div>
                            """
                    # å¦åˆ™ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
                    import time
                    time.sleep(1) # Backoff
        
        # 2. Mock å›é€€ (ä»…åœ¨é»˜è®¤æ¨¡å¼æˆ–æ—  Client æ—¶è§¦å‘)
        if self.mode == LLM_MODE_DEFAULT:
             print("[Info] Using Mock response (Default Mode).")
             return self._mock_response(user_prompt)
        else:
             # Custom æ¨¡å¼ä¸‹å¦‚æœæ²¡æœ‰ Client åˆå§‹åŒ–æˆåŠŸ (æ¯”å¦‚ä¸€å¼€å§‹ Key å°±ç©ºçš„)ï¼Œä¹Ÿè¿”å›é”™è¯¯
             return f"""
             <div style="border: 2px solid #ff9800; padding: 15px; background: #fff8e1; color: #e65100; border-radius: 8px;">
                <h3>âš ï¸ å®¢æˆ·ç«¯æœªåˆå§‹åŒ–</h3>
                <p>è¯·å…ˆåœ¨å·¦ä¾§é…ç½®å¹¶ä¿å­˜æœ‰æ•ˆçš„ API Keyã€‚</p>
             </div>
             """

    def test_connection(self) -> dict:
        """
        æµ‹è¯• API è¿æ¥çŠ¶æ€ (è‡ªæ£€åŠŸèƒ½)ã€‚
        """
        # æ„ä¹‰: éªŒè¯é…ç½®æœ‰æ•ˆæ€§
        # ä½œç”¨: å‘é€æç®€è¯·æ±‚æ£€æµ‹è¿é€šæ€§ï¼Œä¸åæ²¡å¼‚å¸¸
        # å…³è”: å‰ç«¯â€œæµ‹è¯•è¿æ¥â€æŒ‰é’®
        
        if not self.client:
             if self.mode == LLM_MODE_DEFAULT:
                 return {"success": False, "message": "æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ API Keyã€‚è¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ OPENAI_API_KEY æ˜¯å¦è®¾ç½®ã€‚"}
             else:
                 return {"success": False, "message": "å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ã€‚å¯èƒ½æ˜¯ API Key ä¸ºç©ºæˆ– openai åº“æœªå®‰è£…ã€‚"}
        
        # è·å–å®é™…ä½¿ç”¨çš„ Base URL (OpenAI Client ä¼šè‡ªåŠ¨å¤„ç†æœ«å°¾æ–œæ ç­‰)
        actual_url = str(self.client.base_url)
        print(f"[Debug] Testing Connection -> URL: {actual_url}, Key: {self.api_key[:8]}***")

        try:
            # å‘é€ä¸€ä¸ªæç®€çš„æµ‹è¯•è¯·æ±‚
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hi"}],
                max_tokens=5
            )
            model_used = response.model
            return {
                "success": True, 
                "message": f"è¿æ¥æˆåŠŸï¼\n\nâœ… ç›®æ ‡åœ°å€: {actual_url}\nâœ… å“åº”æ¨¡å‹: {model_used}\nâœ… çŠ¶æ€: é€šä¿¡æ­£å¸¸"
            }
        except Exception as e:
            error_msg = str(e)
            print(f"[Debug] Connection Failed: {error_msg}")
            # å°è¯•æå–æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            if "401" in error_msg:
                return {"success": False, "message": f"è®¤è¯å¤±è´¥ (401)ï¼šè¯·æ£€æŸ¥æ‚¨çš„ API Key æ˜¯å¦æ­£ç¡®ã€‚\nè¯¦ç»†ä¿¡æ¯: {error_msg}"}
            elif "404" in error_msg:
                return {"success": False, "message": f"è¯·æ±‚å¤±è´¥ (404)ï¼šå¯èƒ½æ˜¯ API Base URL é”™è¯¯æˆ–æ¨¡å‹åç§°ä¸æ­£ç¡®ã€‚\nç›®æ ‡åœ°å€: {actual_url}\nè¯¦ç»†ä¿¡æ¯: {error_msg}"}
            elif "429" in error_msg:
                return {"success": False, "message": f"è¯·æ±‚è¿‡å¤š (429)ï¼šæ‚¨çš„è´¦æˆ·å¯èƒ½å·²æ¬ è´¹æˆ–è¾¾åˆ°é€Ÿç‡é™åˆ¶ã€‚\nè¯¦ç»†ä¿¡æ¯: {error_msg}"}
            else:
                return {"success": False, "message": f"è¿æ¥æµ‹è¯•å¤±è´¥ï¼š{error_msg}\nç›®æ ‡åœ°å€: {actual_url}"}

    def _mock_response(self, prompt: str) -> str:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤ºã€‚
        """
        print(f"--- [Mock LLM] Mode: {self.mode} ---")
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…ä»¥ç”Ÿæˆç¨å¾®ç›¸å…³çš„ Mock å†…å®¹
        if "å¹´åº¦" in prompt or "summary" in prompt.lower():
            return """
            <h3>å¹´åº¦ç¾¤ç”»åƒ</h3>
            <p><b>ğŸ·ï¸ æ ‡ç­¾ï¼šèµ›åšç²¾ç¥ç—…é™¢</b></p>
            <p>åŸå› ï¼šæ•°æ®è¡¨æ˜ï¼Œæœ¬ç¾¤å¤œé—´æ´»è·ƒåº¦é«˜è¾¾ 80%ï¼Œä¸”â€œå“ˆå“ˆâ€ä¸€è¯å‡ºç°é¢‘ç‡è¿œè¶…äººç±»æ­£å¸¸æ°´å¹³ã€‚</p>
            
            <h3>å­£åº¦å°å‰§åœº (Anime Theater)</h3>
            <p><b>Alice (åæ§½å½¹):</b> è¿™ä¸€å¹´æˆ‘ä»¬åˆ°åº•èŠäº†äº›ä»€ä¹ˆï¼Ÿ</p>
            <p><b>Bob (å¤è¯»æœº):</b> èŠäº†äº›ä»€ä¹ˆï¼Ÿ+1</p>
            <p><b>Charlie (æ½œæ°´å‘˜):</b> ... (å‘å‡ºæŠ¢çº¢åŒ…çš„å£°éŸ³)</p>
            """
        else:
            return f"""
            <h4>å­£åº¦åˆ†ææ‘˜è¦</h4>
            <ul>
            <li><b>æ ¸å¿ƒè¯é¢˜:</b> æ‘¸é±¼ã€æ¸¸æˆã€å¥¶èŒ¶ã€‚</li>
            <li><b>æƒ…æ„Ÿå€¾å‘:</b> æåº¦å¿«ä¹ (Positivity: 0.9)ã€‚</li>
            <li><b>é«˜é¢‘è¯:</b> 666, ç¬‘æ­», æ•‘å‘½ã€‚</li>
            </ul>
            <!-- Debug Info: Input length {len(prompt)} -->
            """

    def build_system_prompt(self, stats_injection: str) -> str:
        """
        æ„å»º System Promptã€‚
        """
        # æ„ä¹‰: Prompt å·¥ç¨‹
        # ä½œç”¨: æ³¨å…¥è§’è‰²è®¾å®šå’Œç¡¬æ€§ç»Ÿè®¡æ•°æ®
        # å…³è”: Phase 2 Statistical Injection
        
        base_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŠå¤©è®°å½•åˆ†æå¸ˆï¼Œæ“…é•¿å¹½é»˜ã€çŠ€åˆ©çš„ç‚¹è¯„ã€‚è¯·æ ¹æ®æä¾›çš„å¯¹è¯å†…å®¹è¿›è¡Œåˆ†æã€‚è¯·ç›´æ¥è¿”å› HTML ç‰‡æ®µï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°ã€‚"
        if stats_injection:
            base_prompt += f"\n\nå‚è€ƒç»Ÿè®¡æ•°æ®ï¼š\n{stats_injection}"
        return base_prompt
