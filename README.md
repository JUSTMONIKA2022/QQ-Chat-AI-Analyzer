# 关于QQ-Chat-AI-Analyzer
这是依赖于Github上的开源项目qq-chat-exporter-master搭建的本地部署的轻量化QQ群聊消息分析工具，出于学习科研目的而开发，可用于生成年度报告等有意思的分析报告。
<br><br>**太过巨大的聊天记录可能会导致分析效果不佳，根据本地测试得到的结论是上传在120mb（推荐值）以下的聊天记录文件能得到比较好的效果。<br>同时请注意，本工具仅做了对qce导出的json格式聊天记录的适配。**
<br><br>毕竟3.0p的注意力挺烂的，如果聊天记录实在超过了推荐值，或许可以手动裁剪聊天记录使得满足推荐值。<br>均匀采样得到的效果我只能说很一般，不如原汁原味喂给ai。

2025.12.28.16:54 更新了自定义的小剧场，但不知道为什么用3.0f经常出不来，还是建议在 *汇总报告模型 (Reduce阶段)* 这里填写3.0p。

2026.1.4.00:17 加上了效果预览，或许在本年度末又能派上用场吧~
> 效果预览：
> https://zhuanlan.zhihu.com/p/1988693559738651264

# 食用指南
> 请详细阅读后进行使用，若有疑问可以联系开发者。阅读大概需要5分钟。
## 这是什么？

这是依赖于***Github***上的开源项目***qq-chat-exporter-master***搭建的**QQ**群聊消息总结分析工具，<br>出于**学习科研目的**而开发。

> 这是***qce***的地址喵~<br>*https://github.com/shuakami/qq-chat-exporter*<br>别忘了点个星星再走哦~

本工具全部由**谷歌**研发的***Gemini-3-pro-preview***实现，创作环境为**字节跳动**旗下的***trae IDE***。<br>是**货真价实的AIGC**哦，因为作者本人是**代码苦手**的说~

此外，本工具调用时可能会把聊天记录完整上传至模型服务提供商，请衡量好**隐私与便利**之间的关系后酌情使用本工具。**若有隐私泄露问题请自行承担责任。**

## 以下是具体的食用教程：

在根目录中双击 *start.bat* 后运行后端服务，打开 *127.0.0.1:5000* 进入前端服务。

### API配置：
负责配置需要调用到的大语言模型，需要用户自行配置模型服务商的**URL地址**、**API key**、**模型名称**。<br>*推荐使用Gemini-3-pro-preview和Gemini-3-flash。*

同时提供了单次分析时可以自由选择模型的选项，以便更好的进行分析。**若不需要该功能，请留空即可。**

### 分析参数：
**Token预算**：取决于你所配置的模型的上下文长度，你填入的模型的上下文长度越大，那么可使用的预算就可以 *手动* 调至更多。例如：
> ***Gemini-3-pro-preview***的上下文长度是 *1M* ，那么你就可以将token预算拉满到 *950k* <br>（长上下文情况下，需要将剩下的50k预算留给提示词与冗余空间）<br>而***官方Deepseek-v3.2***的上下文长度是 *128k* ，那么你就可以将token预算拉到 *128k* 
##### *预算上限暂不支持grok的2M上下文。

**动漫小剧场主题**：可以选择预设的两种主题，也可以自定义主题。选择后可以在报告上生成一段将群友带入该主题角色的小剧场，*纯私货，纯整活，ooc可能性存微。*

**最终输出增强**：开启后在报告生成完毕后再度调用一次LLM，将生成的报告发给LLM将其进行可能存在的**HTML格式修复**与**CSS深度美化**。



### 历史记录：
可以在这里查看所有生成过的分析报告，并提供下载服务。**请注意，所有的分析报告是缓存在本地的。**

### 使用流程：

**第一步：导出数据**
<br>使用 ***QQChatExporter (v5)*** 导出 *JSON* 格式记录。该步骤具体教程参见开头提到的网址。

**第二步：配置 AI**
<br>点击左上角“API 配置”，填入自定义的LLM API配置，以及开启需要的功能。

**第三步：上传分析**
<br>将 JSON 文件拖入中央区域，等待分析完成。

**第四步：获取报告**
<br>进度条走完后，点击下载按钮保存 HTML 报告。

### 感谢 [qq-chat-exporter](https://github.com/shuakami/qq-chat-exporter) 开发者大佬的开源。

### 最后感谢使用本工具喵~<br>若觉得用着顺手记得给个星星哦，若有建议和意见欢迎联系开发者反馈喵。
